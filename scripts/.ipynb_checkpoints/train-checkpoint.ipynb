{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/27/2020 10:08:42 PM: [ COMMAND: /home/sunsi/anaconda3/envs/py35/lib/python3.5/site-packages/ipykernel_launcher.py -f /home/sunsi/.local/share/jupyter/runtime/kernel-36593afa-ca6b-4ee5-8b3e-36b1afda6bba.json ]\n",
      "04/27/2020 10:08:42 PM: [ preprocess_folder = ../data/prepro_dataset/openkp ]\n",
      "04/27/2020 10:08:42 PM: [ Pretrain Model Type = bert-base-cased ]\n",
      "04/27/2020 10:08:47 PM: [ Process rank: -1, device: cuda, n_gpu: 2, distributed training: False, 16-bits training: False ]\n",
      "04/27/2020 10:08:47 PM: [ start setting tokenizer, dataset and dataloader (local_rank = -1)...  ]\n",
      "04/27/2020 10:08:47 PM: [ Model name '../data/pretrain_model/bert-base-cased' not found in model shortcut name list (bert-base-chinese, bert-large-uncased-whole-word-masking, bert-base-cased-finetuned-mrpc, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-german-dbmdz-uncased, bert-base-multilingual-cased, bert-large-uncased-whole-word-masking-finetuned-squad, bert-base-multilingual-uncased, bert-large-cased-whole-word-masking, bert-base-uncased, bert-base-german-cased, bert-base-cased, bert-large-cased, bert-large-uncased, bert-base-german-dbmdz-cased). Assuming '../data/pretrain_model/bert-base-cased' is a path or url to a directory containing tokenizer files. ]\n",
      "04/27/2020 10:08:47 PM: [ Didn't find file ../data/pretrain_model/bert-base-cased/special_tokens_map.json. We won't load it. ]\n",
      "04/27/2020 10:08:47 PM: [ Didn't find file ../data/pretrain_model/bert-base-cased/added_tokens.json. We won't load it. ]\n",
      "04/27/2020 10:08:47 PM: [ Didn't find file ../data/pretrain_model/bert-base-cased/tokenizer_config.json. We won't load it. ]\n",
      "04/27/2020 10:08:47 PM: [ loading file None ]\n",
      "04/27/2020 10:08:47 PM: [ loading file None ]\n",
      "04/27/2020 10:08:47 PM: [ loading file ../data/pretrain_model/bert-base-cased/vocab.txt ]\n",
      "04/27/2020 10:08:47 PM: [ loading file None ]\n",
      "04/27/2020 10:08:47 PM: [ start reloading:  bert2joint (bert) for openkp (train) cached features ... ]\n",
      "6614it [00:08, 806.66it/s]\n",
      "04/27/2020 10:08:55 PM: [ success load 6614 data ]\n",
      "04/27/2020 10:08:55 PM: [ Successfully Preprocess Training Features ! ]\n",
      "04/27/2020 10:08:55 PM: [ start reloading:  bert2joint (bert) for openkp (dev) cached features ... ]\n",
      "6614it [00:08, 760.99it/s]\n",
      "04/27/2020 10:09:04 PM: [ success load 6614 data ]\n",
      "04/27/2020 10:09:04 PM: [ Successfully Preprocess Dev Features ! ]\n",
      "04/27/2020 10:09:04 PM: [  ************************** Initilize Model & Optimizer **************************  ]\n",
      "04/27/2020 10:09:04 PM: [ Training model from scratch... ]\n",
      "04/27/2020 10:09:04 PM: [ Config num_labels = 2 ]\n",
      "04/27/2020 10:09:04 PM: [ loading configuration file ../data/pretrain_model/bert-base-cased/config.json ]\n",
      "04/27/2020 10:09:04 PM: [ Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      " ]\n",
      "04/27/2020 10:09:04 PM: [ loading weights file ../data/pretrain_model/bert-base-cased/pytorch_model.bin ]\n",
      "04/27/2020 10:09:07 PM: [ Weights of BertForChunkTFRanking not initialized from pretrained model: ['cnn2gram.cnn_list.0.bias', 'cnn2gram.cnn_list.0.weight', 'cnn2gram.cnn_list.1.bias', 'cnn2gram.cnn_list.1.weight', 'cnn2gram.cnn_list.2.bias', 'cnn2gram.cnn_list.2.weight', 'cnn2gram.cnn_list.3.bias', 'cnn2gram.cnn_list.3.weight', 'cnn2gram.cnn_list.4.bias', 'cnn2gram.cnn_list.4.weight', 'classifier.bias', 'classifier.weight', 'chunk_classifier.bias', 'chunk_classifier.weight'] ]\n",
      "04/27/2020 10:09:07 PM: [ Weights from pretrained model not used in BertForChunkTFRanking: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias'] ]\n",
      "04/27/2020 10:09:07 PM: [ warmup steps : 55 ]\n",
      "04/27/2020 10:09:12 PM: [ Training/evaluation parameters Namespace(adam_epsilon=1e-08, cache_dir='../data/pretrain_model/bert-base-cased', cached_features_dir='../data/cached_features', checkpoint_file='', checkpoint_folder='../results/train_bert2joint_openkp_bert_04.27_22.08/checkpoints', cuda=True, data_workers=0, dataset_class='openkp', device=device(type='cuda'), display_iter=200, eval_checkpoint='', fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=2, learning_rate=5e-05, load_checkpoint=False, local_rank=-1, log_file='../results/train_bert2joint_openkp_bert_04.27_22.08/logging.txt', max_grad_norm=1.0, max_phrase_words=5, max_token=510, max_train_epochs=1, max_train_steps=0, model_class='bert2joint', n_gpu=2, no_cuda=False, num_labels=2, per_gpu_test_batch_size=32, per_gpu_train_batch_size=3, preprocess_folder='../data/prepro_dataset/openkp', pretrain_model_path='../data/pretrain_model', pretrain_model_type='bert-base-cased', run_mode='train', save_checkpoint=True, save_folder='../results/train_bert2joint_openkp_bert_04.27_22.08', save_path='../results', seed=42, server_ip='', server_port='', tag_pooling='min', test_batch_size=64, train_batch_size=6, use_viso=False, warmup_proportion=0.1, weight_decay=0.01) ]\n",
      "04/27/2020 10:09:12 PM: [  ************************** Running training **************************  ]\n",
      "04/27/2020 10:09:12 PM: [   Num Train examples = 6614 ]\n",
      "04/27/2020 10:09:12 PM: [   Num Train Epochs = 1 ]\n",
      "04/27/2020 10:09:12 PM: [   Instantaneous batch size per GPU = 3 ]\n",
      "04/27/2020 10:09:12 PM: [   Total train batch size (w. parallel, distributed & accumulation) = 12 ]\n",
      "04/27/2020 10:09:12 PM: [   Gradient Accumulation steps = 2 ]\n",
      "04/27/2020 10:09:12 PM: [   Total optimization steps = 551 ]\n",
      "04/27/2020 10:09:12 PM: [  ***********************************************************************  ]\n",
      "04/27/2020 10:09:12 PM: [ start training bert2joint on openkp (1 epoch) || local_rank = -1... ]\n",
      "Train_Iteration:   0%|          | 0/1103 [00:00<?, ?it/s]/home/sunsi/anaconda3/envs/py35/lib/python3.5/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "04/27/2020 10:09:20 PM: [ Local Rank = -1 | train: Epoch = 1 | iter = 0/1103 | loss = 0.8488 | lr = 0.000000 | 0 updates | elapsed time = 8.12 (s) \n",
      " ]\n",
      "Train_Iteration:   8%|â–Š         | 92/1103 [01:42<15:42,  1.07it/s] "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = ','.join(map(str, [0,1]))\n",
    "import sys\n",
    "import time\n",
    "import tqdm\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import logging\n",
    "import argparse\n",
    "import traceback\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "import test\n",
    "import utils\n",
    "import config\n",
    "from model import KeyphraseSpanExtraction\n",
    "from utils import pred_arranger, pred_saver\n",
    "from bertkpe import dataloader, generator, evaluator\n",
    "from bertkpe import tokenizer_class, Idx2Tag, Tag2Idx, Decode_Candidate_Number\n",
    "\n",
    "torch.backends.cudnn.benchmark=True\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# -------------------------------------------------------------------------------------------\n",
    "# Trainer\n",
    "# -------------------------------------------------------------------------------------------\n",
    "def train(args, data_loader, model, train_input_refactor,stats, writer):\n",
    "    logger.info(\"start training %s on %s (%d epoch) || local_rank = %d...\" %\n",
    "                (args.model_class, args.dataset_class, stats['epoch'], args.local_rank))\n",
    "    \n",
    "    train_loss = utils.AverageMeter()\n",
    "    epoch_time = utils.Timer()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_step = 0\n",
    "    \n",
    "    epoch_iterator = tqdm(data_loader, desc=\"Train_Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        \n",
    "        if step > 100:break\n",
    "        \n",
    "        inputs, indices = train_input_refactor(batch, model.args.device)\n",
    "        try:\n",
    "            loss = model.update(step, inputs)\n",
    "        except:\n",
    "            logging.error(str(traceback.format_exc()))\n",
    "            continue\n",
    "            \n",
    "        train_loss.update(loss)\n",
    "        epoch_loss += loss\n",
    "        epoch_step += 1\n",
    "        \n",
    "        if args.local_rank in [-1, 0] and step % args.display_iter == 0:\n",
    "            if args.use_viso:\n",
    "                writer.add_scalar('train/loss', train_loss.avg, model.updates)\n",
    "                writer.add_scalar('train/lr', model.scheduler.get_lr()[0], model.updates)\n",
    "\n",
    "            logging.info('Local Rank = %d | train: Epoch = %d | iter = %d/%d | ' %\n",
    "                        (args.local_rank, stats['epoch'], step, len(train_data_loader)) +\n",
    "                        'loss = %.4f | lr = %f | %d updates | elapsed time = %.2f (s) \\n' %\n",
    "                        (train_loss.avg, model.scheduler.get_lr()[0], model.updates, stats['timer'].time()))\n",
    "            train_loss.reset()\n",
    "        \n",
    "    logging.info('Local Rank = %d | Epoch Mean Loss = %.8f ( Epoch = %d ) | Time for epoch = %.2f (s) \\n' %\n",
    "                 (args.local_rank, (epoch_loss / epoch_step), stats['epoch'], epoch_time.time()))\n",
    "        \n",
    "    \n",
    "# -------------------------------------------------------------------------------------------\n",
    "# Main Function\n",
    "# -------------------------------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # setting args\n",
    "    parser = argparse.ArgumentParser('BertKPE', formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    config.add_default_args(parser)\n",
    "    \n",
    "\n",
    "    # *****************************************************************************\n",
    "    parameters = \"--run_mode train \\\n",
    "                  --max_train_epochs 1 \\\n",
    "                  --local_rank -1 \\\n",
    "                  --model_class bert2joint \\\n",
    "                  --pretrain_model_type bert-base-cased \\\n",
    "                  --per_gpu_train_batch_size 3 \\\n",
    "                  --per_gpu_test_batch_size 32 \\\n",
    "                  --dataset_class openkp \\\n",
    "                  --save_checkpoint \\\n",
    "                  --preprocess_folder ../data/prepro_dataset \\\n",
    "                  --pretrain_model_path ../data/pretrain_model \\\n",
    "                  --cached_features_dir ../data/cached_features \\\n",
    "                  --save_path ../results\"\n",
    "    args = parser.parse_args(parameters.split())\n",
    "    # *****************************************************************************\n",
    "    \n",
    "#     args = parser.parse_args()\n",
    "    config.init_args_config(args)\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Setup distant debugging if needed\n",
    "    if args.server_ip and args.server_port:\n",
    "        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n",
    "        import ptvsd\n",
    "        print(\"Waiting for debugger attach\")\n",
    "        ptvsd.enable_attach(address=(args.server_ip, args.server_port), redirect_output=True)\n",
    "        ptvsd.wait_for_attach()\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Setup CUDA, GPU & distributed training\n",
    "    args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        args.n_gpu = torch.cuda.device_count()\n",
    "    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "        args.n_gpu = 1\n",
    "    args.device = device\n",
    "    logger.info(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n",
    "                args.local_rank, device, args.n_gpu, bool(args.local_rank != -1), args.fp16)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    utils.set_seed(args)\n",
    "    # Make sure only the first process in distributed training will download model & vocab\n",
    "        \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # init tokenizer & Converter \n",
    "    logger.info(\"start setting tokenizer, dataset and dataloader (local_rank = {})... \".format(args.local_rank))\n",
    "    tokenizer = tokenizer_class[args.pretrain_model_type].from_pretrained(args.cache_dir)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Select dataloader\n",
    "    batchify_features_for_train, batchify_features_for_test = dataloader.get_class(args.model_class)\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # build train dataloader\n",
    "    train_dataset = dataloader.build_dataset(**{'args':args, 'tokenizer':tokenizer, 'mode':'train'})\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "    train_sampler = torch.utils.data.sampler.RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=args.data_workers,\n",
    "        collate_fn=batchify_features_for_train,\n",
    "        pin_memory=args.cuda,\n",
    "    )\n",
    "    logger.info(\"Successfully Preprocess Training Features !\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # build dev dataloader \n",
    "    dev_dataset = dataloader.build_dataset(**{'args':args, 'tokenizer':tokenizer, 'mode':'dev'})\n",
    "    args.test_batch_size = args.per_gpu_test_batch_size * max(1, args.n_gpu)\n",
    "    dev_sampler = torch.utils.data.sampler.SequentialSampler(dev_dataset)\n",
    "    dev_data_loader = torch.utils.data.DataLoader(\n",
    "        dev_dataset,\n",
    "        batch_size=args.test_batch_size,\n",
    "        sampler=dev_sampler,\n",
    "        num_workers=args.data_workers,\n",
    "        collate_fn=batchify_features_for_test,\n",
    "        pin_memory=args.cuda,\n",
    "    )\n",
    "    logger.info(\"Successfully Preprocess Dev Features !\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # build eval dataloader \n",
    "    if args.dataset_class == 'kp20k':\n",
    "        eval_dataset = dataloader.build_dataset(**{'args':args, 'tokenizer':tokenizer, 'mode':'eval'})\n",
    "        eval_sampler = torch.utils.data.sampler.SequentialSampler(eval_dataset)\n",
    "        eval_data_loader = torch.utils.data.DataLoader(\n",
    "            eval_dataset,\n",
    "            batch_size=args.test_batch_size,\n",
    "            sampler=eval_sampler,\n",
    "            num_workers=args.data_workers,\n",
    "            collate_fn=batchify_features_for_test,\n",
    "            pin_memory=args.cuda,\n",
    "        )\n",
    "        logger.info(\"Successfully Preprocess Eval Features !\")\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Set training total steps\n",
    "    if args.max_train_steps > 0:\n",
    "        t_total = args.max_train_steps\n",
    "        args.max_train_epochs = args.max_train_steps // (len(train_data_loader) // args.gradient_accumulation_steps) + 1\n",
    "    else:\n",
    "        t_total = len(train_data_loader) // args.gradient_accumulation_steps * args.max_train_epochs\n",
    "        \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Preprare Model & Optimizer\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    if args.local_rank not in [-1, 0]:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "        \n",
    "    logger.info(\" ************************** Initilize Model & Optimizer ************************** \")\n",
    "    \n",
    "    if args.load_checkpoint and os.path.isfile(args.checkpoint_file):\n",
    "        model, checkpoint_epoch = KeyphraseSpanExtraction.load_checkpoint(args.checkpoint_file, args)        \n",
    "    else:\n",
    "        logger.info('Training model from scratch...')\n",
    "        model = KeyphraseSpanExtraction(args)\n",
    "        \n",
    "    # initial optimizer\n",
    "    model.init_optimizer(num_total_steps=t_total)\n",
    "    \n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    if args.local_rank == 0:\n",
    "        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # set model device\n",
    "    model.set_device()\n",
    "    \n",
    "    if args.n_gpu > 1:\n",
    "        model.parallelize()\n",
    "    \n",
    "    if args.local_rank != -1:\n",
    "        model.distribute()\n",
    "\n",
    "    if args.local_rank in [-1, 0] and args.use_viso:\n",
    "        tb_writer = SummaryWriter(args.viso_folder)\n",
    "    else:\n",
    "        tb_writer = None\n",
    "        \n",
    "    logger.info(\"Training/evaluation parameters %s\", args)\n",
    "    logger.info(\" ************************** Running training ************************** \")\n",
    "    logger.info(\"  Num Train examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Train Epochs = %d\", args.max_train_epochs)\n",
    "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n",
    "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
    "                args.train_batch_size * args.gradient_accumulation_steps * (torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "    logger.info(\" *********************************************************************** \")\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------\n",
    "    # Method Select\n",
    "    candidate_decoder = test.select_decoder(args.model_class)\n",
    "    evaluate_script, main_metric_name = utils.select_eval_script(args.dataset_class)\n",
    "    train_input_refactor, test_input_refactor = utils.select_input_refactor(args.model_class)\n",
    " \n",
    "    stats = {'timer': utils.Timer(), 'epoch': 0, main_metric_name: 0}\n",
    "    for epoch in range(1, (args.max_train_epochs+1)):\n",
    "        stats['epoch'] = epoch\n",
    "        \n",
    "        # train \n",
    "        train(args, train_data_loader, model, train_input_refactor, stats, tb_writer)\n",
    "\n",
    "        # previous metric score\n",
    "        prev_metric_score = stats[main_metric_name]\n",
    "        \n",
    "        # decode candidate phrases\n",
    "        dev_candidate = candidate_decoder(args, dev_data_loader, dev_dataset, model, test_input_refactor, pred_arranger, 'dev')\n",
    "        stats = evaluate_script(args, dev_candidate, stats, mode='dev', metric_name=main_metric_name)\n",
    "            \n",
    "        # new metric score\n",
    "        new_metric_score = stats[main_metric_name]\n",
    "            \n",
    "        # save checkpoint : when new metric score > previous metric score\n",
    "        if args.save_checkpoint and (new_metric_score > prev_metric_score) and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n",
    "            checkpoint_name = '{}.{}.{}.epoch_{}.checkpoint'.format(args.model_class, args.dataset_class, args.pretrain_model_type.split('-')[0], epoch)\n",
    "            model.save_checkpoint(os.path.join(args.checkpoint_folder, checkpoint_name), stats['epoch'])\n",
    "        \n",
    "        # -----------------------------------------------------------------------------------------------------------------------------\n",
    "        # eval evaluation\n",
    "        if args.dataset_class == 'kp20k':\n",
    "            eval_candidate = candidate_decoder(args, eval_data_loader, eval_dataset, model, test_input_refactor, pred_arranger, 'eval')\n",
    "            eval_stats = {'epoch': epoch, main_metric_name: 0}\n",
    "            eval_stats = evaluate_script(args, eval_candidate, eval_stats, mode='eval', metric_name=main_metric_name)\n",
    "        # -----------------------------------------------------------------------------------------------------------------------------\n",
    "        \n",
    "        \n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
